# services/google_trends_explore_service.py
import os
import re
from bs4 import BeautifulSoup

from parsers.google_trend_explore_parser import GoogleTrendsExploreParser
from anti_bot.anti_bot_browser import AntiBotBrowser
from sniffer.sniffer import NetworkTap, TapRule, TapContextForBrowserContext


class GoogleTrendsExploreService:
    def __init__(self, url: str, headless: bool = False):
        self.url = url
        self.headless = headless

    def get_explore_data(
        self,
        timeout_ms: int = 15_000,
        enable_sniffer: bool = True,
        sniffer_out: str = "tap_out",
    ):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
          - –ø—Ä–∏ enable_sniffer=True: (payload_dict, tap)
          - –∏–Ω–∞—á–µ: payload_dict
        """
        tap = None

        # üéõÔ∏è –ø—Ä–∞–≤–∏–ª–∞ —Å–Ω–∏—Ñ—Ñ–µ—Ä–∞ ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ ¬´–≤—Å—ë –ø–∏—à–∏, –≤—Å—ë –ø—Ä–æ–ø—É—Å–∫–∞–π¬ª
        rule = TapRule(
            debug=True,
            allow_ctypes=None,            # –Ω–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ Content-Type
            allow_resource_types=None,    # –Ω–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ —Ç–∏–ø—É —Ä–µ—Å—É—Ä—Å–∞
            allow_url=None,
            deny_url=None,                # –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –Ω–∏—á–µ–≥–æ
            max_bytes=1_000_000_000,      # –≤—ã—Å–æ–∫–∏–π –ø–æ—Ç–æ–ª–æ–∫ —Ä–∞–∑–º–µ—Ä–∞ (1 –ì–ë)
            skip_sse=False,               # –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å SSE (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å –æ–±—ä—ë–º–æ–º)
            save_binary=True,             # —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –±–∏–Ω–∞—Ä—å
            gzip_text=False,              # –ø–∏—Å–∞—Ç—å .json/.txt –±–µ–∑ gzip (—É–¥–æ–±–Ω–µ–µ —Å–º–æ—Ç—Ä–µ—Ç—å)
            save_request_body=True,
            request_body_max=1_000_000,   # –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å POST (f.req) —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ
            decode_batchexecute=True,     # —Ä–∞–∑–±–æ—Ä Google batchexecute
            index_path=None,              # –±–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ (–≤–∫–ª—é—á–∏ –ø—Ä–∏ –Ω—É–∂–¥–µ)
            emit_curl=True,
        )

        with AntiBotBrowser(headless=self.headless) as bot:
            if enable_sniffer:
                tap = NetworkTap(
                    out_dir=sniffer_out,
                    rule=rule,
                    dedup_db=os.path.join(".tap_cache", "seen.sqlite"),  # –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π –¥–µ–¥—É–ø
                )
                tap.rule.deny_url = None  # —É–±—Ä–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –±–∞–Ω api.ipify.org (–µ—Å–ª–∏ –±—ã–ª)

                # üì° –ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫–æ –í–°–ï–ú —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –î–û –ø–µ—Ä–µ—Ö–æ–¥–∞
                with TapContextForBrowserContext(bot.context, tap):
                    bot.goto(self.url)
                    self._kick_explore_requests(bot.page)
                    html = self._wait_for_explore_data(bot.page, timeout_ms)
            else:
                bot.goto(self.url)
                self._kick_explore_requests(bot.page)
                html = self._wait_for_explore_data(bot.page, timeout_ms)

        # üß© –ø–∞—Ä—Å–∏–º HTML –∫–∞–∫ —Ä–∞–Ω—å—à–µ
        parser = GoogleTrendsExploreParser(html)
        payload = {
            "popularity_timeseries": parser.get_popularity_timeseries(),
            "regions": parser.get_regions(),
            "topics": parser.get_topics(),
            "related_queries": parser.get_related_queries(),
        }
        return (payload, tap) if enable_sniffer else payload

    # ‚Äî‚Äî‚Äî –ø–æ–º–æ—â–Ω–∏–∫–∏ ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

    def _kick_explore_requests(self, page) -> None:
        """–ß—É—Ç—å ¬´–ø–∏–Ω–∞–µ–º¬ª —Å—Ç—Ä–∞–Ω–∏—Ü—É, —á—Ç–æ–±—ã –æ—Ç—Å—Ç—Ä–µ–ª—è–ª–∏—Å—å –≤—Å–µ –ª–µ–Ω–∏–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã."""
        try:
            page.wait_for_load_state("networkidle", timeout=10_000)
        except Exception:
            pass
        try:
            for _ in range(3):
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(900)
        except Exception:
            pass

    def _wait_for_explore_data(self, page, timeout_ms: int = 10_000) -> str:
        """
        –ñ–¥—ë–º, –ø–æ–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ—è–≤–∏—Ç—Å—è —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –Ω–∞–¥—ë–∂–Ω—ã–π –º–∞—Ä–∫–µ—Ä:
         ‚Ä¢ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –ª–∏–Ω–µ–π–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ (svg),
         ‚Ä¢ —Ç–∞–±–ª–∏—Ü–∞ ¬´—Ç–∞–±–ª–∏—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ / table view¬ª,
         ‚Ä¢ –±–ª–æ–∫–∏ ¬´–ü–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã/Related queries¬ª, ¬´–ï—â—ë –ø–æ —Ç–µ–º–µ/Related topics¬ª,
           ¬´–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–æ —Å—É–±—Ä–µ–≥–∏–æ–Ω–∞–º/Interest by subregion¬ª.
        """
        interval = 500
        attempts = max(1, timeout_ms // interval)

        label_re = re.compile(
            r"(–ï—â—ë –ø–æ —Ç–µ–º–µ|Related topics|–ü–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã|Related queries|"
            r"–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–æ —Å—É–±—Ä–µ–≥–∏–æ–Ω–∞–º|Interest by subregion)",
            re.I,
        )

        for _ in range(attempts):
            html = page.content()
            soup = BeautifulSoup(html, "html.parser")

            has_chart_svg = soup.select_one("div.fe-line-chart-content-container svg")
            has_chart_table = (
                soup.select_one("div[aria-label*='—Ç–∞–±–ª–∏—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ' i] table")
                or soup.select_one("div[aria-label*='table view' i] table")
            )
            has_label = soup.find(string=label_re)

            if has_chart_svg or has_chart_table or has_label:
                return html

            page.wait_for_timeout(interval)

        raise TimeoutError("‚ùå –ù–µ –¥–æ–∂–¥–∞–ª–∏—Å—å Explore-–¥–∞–Ω–Ω—ã—Ö Google Trends.")
