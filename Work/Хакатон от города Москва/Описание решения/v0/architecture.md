# Архитектура проекта с точки зрения Java и Python

## Введение

Проект представляет собой систему, которая обрабатывает данные с технологических объектов (например, насосных станций, водопроводных и канализационных сетей) и принимает решения по возможным рискам и аварийным ситуациям. Система использует гибридную аналитику: **Java** обрабатывает потоки данных в реальном времени, а **Python** отвечает за машинное обучение (ML) и симуляции на базе цифрового двойника.

## Структура проекта

Проект состоит из нескольких слоёв, каждый из которых отвечает за определённую часть логики и обработки данных. На каждом уровне задействованы сервисы, реализованные как микросервисы. Микросервисы могут быть реализованы на **Java** или **Python**, в зависимости от задачи.

### 1. **Ingest Gate** (Java, Spring Boot)

**Что делает:**

- Принимает данные с различных источников (SCADA, OPC UA, REST, файлы).
- Приводит данные к унифицированному формату.
- Обрабатывает качество данных (флаги «valid», «stale», «range»).
- Публикует данные в Kafka в формате Avro или Proto.

**Почему Java?**
Java — это производительный язык для высокоскоростной обработки данных и работы с большими потоками событий. Spring Boot предоставляет удобные средства для создания REST-сервисов и интеграции с Kafka.

**Пример работы:**  
Принимаем данные о давлении и расходе с объекта (например, насосной станции). Данные могут быть из различных форматов (CSV, REST API, Modbus). Система приводит все значения к единому формату и публикует их в Kafka.

### 2. **Stream Processor (Data Quality & Enrichment)** (Java, Flink)

**Что делает:**

- Применяет фильтры для очистки данных: удаляет выбросы, обнаруживает «залипания» и «дрейф» (например, при потере связи).
- Выполняет агрегации по окнам времени (например, среднее значение за 5 минут).
- Создаёт агрегированные данные для дальнейшей обработки.

**Почему Java/Flink?**
Flink — это потоковая обработка данных в реальном времени, и на базе Java можно легко интегрировать эти решения с Kafka и обеспечивать масштабируемость при обработке больших объёмов данных.

**Пример работы:**  
Принимаем данные с датчика давления и расхода, проводим агрегацию по окнам 5 минут, вычисляем наклон (slope) и z-score, чтобы определить аномалии.

### 3. **CEP + Rules Engine (Java, Drools)**

**Что делает:**

- Обрабатывает данные и применяет бизнес-правила (например, «падение давления + рост расхода» — может указывать на утечку).
- Использует Drools для выполнения комплексных событийных паттернов (CEP).
- Публикует события в Kafka, если срабатывают правила.

**Почему Java/Drools?**
Drools — это мощный движок для обработки бизнес-правил, который хорошо интегрируется с Java. Он подходит для реализации сложных логических правил и условий.

**Пример работы**  
Применяется правило: если давление падает ниже 3.5 бар и расход увеличивается, создаём тревогу об утечке. Эти данные публикуются в Kafka.

### 4. **Feature Service / Feature Store** (Java, Redis/PostgreSQL)

**Что делает:**

- Хранит признаки (features) для ML-моделей.
- Выдаёт признаки онлайн для каждого сигнала в реальном времени.
- Хранит исторические данные для обучения моделей.

**Почему Java/Redis/PostgreSQL?**
Redis — это быстрый кэш, который подходит для хранения горячих данных, которые часто запрашиваются. PostgreSQL — это реляционная СУБД, которая используется для хранения более стабильных данных.

**Пример работы:**  
Для датчика давления вычисляются агрегированные признаки (например, скользящее среднее за 5 минут, наклон за 15 минут). Эти признаки используются для дальнейшего прогноза в модели.

### 5. **ML Inference** (Python, FastAPI, gRPC)

**Что делает**

- Прогнозирует возможные аномалии или будущие значения (например, предсказывает падение давления).
- Возвращает прогнозы и оценку аномалии (например, вероятность утечки).
- Обрабатывает запросы через FastAPI с использованием gRPC для взаимодействия с другими сервисами.

**Почему Python?**
Python — это стандарт для разработки и обучения моделей машинного обучения (ML). Он обладает богатым набором библиотек для работы с данными, обучением моделей и их инференсом.

**Пример работы:**  
Система получает признаки от Feature Service и делает прогноз для следующего временного окна (например, через 10 минут давление упадёт до 3.0 бар).

### 6. **Digital Twin (What-If Simulation)** (Python, FastAPI)

**Что делает:**

- Выполняет симуляцию «что будет, если» (например, что произойдёт, если мы уменьшили обороты насоса на 10%).
- Проверяет, как изменения в системе повлияют на работу сети.
- Возвращает результаты симуляции для дальнейшей оценки.

**Почему Python?**
Для работы с цифровыми двойниками и моделями часто используется Python благодаря его богатым библиотекам для симуляций и вычислений.

**Пример работы:**  
Модели гидравлики (например, EPANET) анализируют влияние снижения оборотов насоса на давление в трубах.

### 7. **Recommendation Engine** (Java)

**Что делает:**

- Сводит результаты с разных слоёв: правила, статистика, ML и симуляции (digital twin).
- Ранжирует рекомендации по степени важности и выдаёт диспетчеру карточку с рекомендациями.
- Оценивает уверенность в рекомендациях, чтобы диспетчер мог принять решение.

**Почему Java?**
Java используется для сводки всех доказательств в одно место и обработки больших потоков данных. Он отлично подходит для создания системы рекомендаций.

**Пример работы:**  
После того как в систему поступают данные о падении давления и увеличении расхода, а также прогнозы из ML и цифрового двойника, система генерирует рекомендацию: уменьшить обороты насоса и открыть задвижку для восстановления давления.

### 8. **UI (Frontend)** (React/TypeScript)

**Что делает:**

- Отображает ленту событий и рекомендации диспетчерам.
- Показывает графики, показатели и кнопки для выполнения рекомендаций.
- Получает фидбек от пользователей (диспетчеров) и передает их обратно в систему для дальнейшего обучения.

**Почему React/TypeScript?**
React позволяет быстро и эффективно обновлять интерфейс при изменении данных. TypeScript обеспечивает строгую типизацию, что минимизирует ошибки на этапе разработки.

**Пример работы:**  
Диспетчер видит предупреждение о низком давлении, получает рекомендацию по снижению оборотов насоса и нажимает кнопку «Выполнено».

---

## Взаимодействие между Java и Python сервисами

Всё взаимодействие между сервисами организовано через **gRPC** и **REST** API. Сервис на **Java** обрабатывает данные в реальном времени (например, агрегация и проверка по правилам), а сервисы на **Python** отвечают за более сложные вычисления, такие как **ML-прогнозы** и **симуляции цифрового двойника**.

- **gRPC** используется для передачи данных между сервисами с низкой задержкой.
- **REST API** используется для получения данных и выдачи рекомендаций.

## Пример процесса:

1. **Ingest Gate** принимает данные с источников и публикует их в Kafka.
2. **Stream Processor** очищает данные, применяет агрегации и проверку качества, отправляет результат в Kafka.
3. **CEP + Rules Engine** применяет бизнес-правила (например, обнаружение утечек) и публикует результат в Kafka.
4. **Feature Service** извлекает признаки для машинного обучения и сохраняет их в базе данных.
5. **ML Inference** на основе признаков делает прогнозы, возвращает результаты через gRPC.
6. **Digital Twin** выполняет симуляцию «что будет, если» и передает результаты.
7. **Recommendation Engine** собирает все данные и генерирует рекомендации, которые передаются в UI.
8. **UI** отображает рекомендации и события диспетчерам.

---

## Заключение

Проект включает в себя сложную архитектуру с микросервисами, которые выполняют различные задачи по обработке и анализу данных, прогнозированию возможных аномалий, а также генерации рекомендаций для диспетчеров. Каждый сервис, реализованный на **Java** или **Python**, выполняет свою роль в этой экосистеме.
